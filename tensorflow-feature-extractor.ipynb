{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "Named Entity Recognition(NER) is the process of identifying named entities in text. Some examples of named entities are: 'Person', 'Time', 'Location', 'Organization'. NER is essentially a token classification task where every token is classified into one or more predetermined categories.\n",
    "\n",
    "In this exercise, I will train a simple Transformer based model to perform NER. I will use data from the Kaggle competition: 'NBME - Score Clinical Patient Notes'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import string\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from datasets import load_dataset\n",
    "from collections import Counter\n",
    "from acquire import prep_and_split_data\n",
    "from acquire import basic_clean_v2\n",
    "from acquire import basic_clean_features\n",
    "from conlleval import evaluate  \n",
    "\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, GlobalAveragePooling1D\n",
    "from tensorflow.keras.layers import TextVectorization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will use the transformer implementation from this fantastic [example](https://keras.io/examples/nlp/text_classification_with_transformer/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define a TransformerBlock layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(layers.Layer):\n",
    "  def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "    super(TransformerBlock, self).__init__()\n",
    "    self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "    self.ffn = keras.Sequential(\n",
    "        [layers.Dense(ff_dim, activation='relu'), layers.Dense(embed_dim)]\n",
    "    )\n",
    "    self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "    self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "    self.dropout1 = layers.Dropout(rate)\n",
    "    self.dropout2 = layers.Dropout(rate)\n",
    "\n",
    "  def call(self, inputs, training):\n",
    "    attn_output = self.att(inputs, inputs)\n",
    "    attn_output = self.dropout1(attn_output, training=training)\n",
    "    out1 = self.layernorm1(inputs + attn_output)\n",
    "    ffn_output = self.ffn(out1)\n",
    "    ffn_output = self.dropout2(ffn_output, training=training)\n",
    "    return self.layernorm2(out1 + ffn_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define a TokenAndPositionEmbedding layer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will use the `TokenAndPositionEmbedding` layer from [ner transformer](https://colab.research.google.com/github/keras-team/keras-io/blob/master/examples/nlp/ipynb/ner_transformers.ipynb#scrollTo=YY0STTK9sKP3):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenAndPositionEmbedding(layers.Layer):\n",
    "  def __init__(self, maxlen, vocab_size, embed_dim):\n",
    "    super(TokenAndPositionEmbedding, self).__init__()\n",
    "    self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
    "    self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
    "\n",
    "  def call(self, inputs):\n",
    "    maxlen = tf.shape(inputs)[-1]\n",
    "    positions = tf.range(start=0, limit=maxlen, delta=1)\n",
    "    position_embeddings = self.pos_emb(positions)\n",
    "    token_embeddings = self.token_emb(inputs)\n",
    "    return token_embeddings + position_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the NER model class as a `keras.Model` subclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NERModel(keras.Model):\n",
    "    def __init__(\n",
    "        self, num_tags, vocab_size, maxlen=170, embed_dim=170, num_heads=3, ff_dim=170\n",
    "    ):\n",
    "        super(NERModel, self).__init__()\n",
    "        self.embedding_layer = TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim)\n",
    "        self.transformer_block = TransformerBlock(embed_dim, num_heads, ff_dim)\n",
    "        self.dropout1 = layers.Dropout(0.1)\n",
    "        self.ff = layers.Dense(ff_dim, activation='relu')\n",
    "        self.dropout2 = layers.Dropout(0.1)\n",
    "        self.ff_final = layers.Dense(num_tags)\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        x = self.embedding_layer(inputs)\n",
    "        x = self.transformer_block(x)\n",
    "        x = self.dropout1(x, training=training)\n",
    "        x = self.ff(x)\n",
    "        x = self.dropout2(x, training=training)\n",
    "        x = self.ff_final(x)\n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data from 'features.csv' and 'patient_notes.csv'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in training set: 37931\n",
      "Number of rows in validation set: 2108\n",
      "Number of rows in test set: 2107\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda3/lib/python3.8/site-packages/pandas/core/frame.py:4308: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  return super().drop(\n"
     ]
    }
   ],
   "source": [
    "train, validate, test = prep_and_split_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Here I will manually create a word embedding layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the Embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embed a batch of 7000 words with ovservations of length 163.\n",
    "embedding_layer = tf.keras.layers.Embedding(7000, 163)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find a reasonable number for the vocab size and sequence length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_words = pd.Series(' '.join(train.basic_clean_v2).split()).unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counts = pd.Series(' '.join(train.basic_clean_v2).split()).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       ",          429737\n",
       ".          372297\n",
       ":          178467\n",
       "and        126653\n",
       "no         114796\n",
       "            ...  \n",
       "values         10\n",
       "wee            10\n",
       "pots           10\n",
       "(when          10\n",
       "familiy        10\n",
       "Length: 7215, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_counts[word_counts >= 10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are roughly 7000 vocabulary words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I will define the dataset preprocessing steps. I will initialize a TextVectorization layer with my desired parameters to vectorize the student notes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-27 18:05:16.397205: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# Create a custom standardization function.\n",
    "def custom_standardization(input_data):\n",
    "  lowercase = tf.strings.lower(input_data)\n",
    "  return tf.strings.regex_replace(lowercase,\n",
    "                                  '[%s]' % re.escape(string.punctuation), '')\n",
    "\n",
    "\n",
    "# Vocabulary size and number of words in a sequence.\n",
    "vocab_size = 7000\n",
    "sequence_length = 163\n",
    "\n",
    "# Use the text vectorization layer to normalize, split, and map strings to\n",
    "# integers. Note that the layer uses the custom standardization defined above.\n",
    "# Set maximum_sequence length as all samples are not of the same length.\n",
    "vectorize_layer = TextVectorization(\n",
    "    standardize = None,\n",
    "    max_tokens = vocab_size,\n",
    "    output_mode = 'int',\n",
    "    output_sequence_length = sequence_length)\n",
    "\n",
    "# Make a text-only dataset (no labels) and call adapt to build the vocabulary.\n",
    "text = [x for x in train.basic_clean_v2]\n",
    "text_ds = tf.data.Dataset.from_tensor_slices(text)\n",
    "vectorize_layer.adapt(text_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(37931, 163), dtype=int64, numpy=\n",
       "array([[ 323,   46,   74, ..., 1421,    2,   81],\n",
       "       [2488,  188,    7, ...,  130,   39,    2],\n",
       "       [  41,   15,   11, ...,    0,    0,    0],\n",
       "       ...,\n",
       "       [1029,    3,  148, ...,   56,    9,  281],\n",
       "       [ 226,   46,  116, ...,    6,  372,   94],\n",
       "       [ 323,   46,   74, ...,    2,  357,    3]])>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorize_layer(train.basic_clean_v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_notes = vectorize_layer(text).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_split = []\n",
    "for note in text:\n",
    "    text_split.append(note.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export the data to a tab-separated file format which will be easy to read as a `tf.data.Dataset` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_to_file(export_file_path, data):\n",
    "    with open(export_file_path, \"w\") as f:\n",
    "        for i in np.arange(len(data)):\n",
    "            ner_tags = encoded_notes[i]\n",
    "            tokens = text_split[i]\n",
    "            f.write(\n",
    "                str(len(tokens))\n",
    "                + \"\\t\"\n",
    "                + \"\\t\".join(tokens)\n",
    "                + \"\\t\"\n",
    "                + \"\\t\".join(map(str, ner_tags))\n",
    "                + \"\\n\"\n",
    "            )\n",
    "\n",
    "# export_to_file(\"./data/train.txt\", train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maybe set the targets as the 'ner_labels'\n",
    "Make a single list fo all unique targets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_targets = []\n",
    "for list_of_ailments in train.targets:\n",
    "    for ailment in list_of_ailments:\n",
    "        list_of_targets.append(ailment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_targets = pd.Series(list_of_targets).unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['increased appetite', 'son died 3 weeks ago', 'female',\n",
       "       'auditory hallucination once', 'tossing and turning'], dtype=object)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_of_targets[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make the NER label lookup table\n",
    "\n",
    "NER labels are usually provided in IOB, IOB2 or IOBES formats. Checkout this link for more information: [Wikipedia](\"https://en.wikipedia.org/wiki/Inside–outside–beginning_(tagging\")\n",
    "\n",
    "I will start the label numbering from 1 since 0 is reserved for padding. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: '[PAD]', 1: 'O', 2: 'B-increased appetite', 3: 'I-increased appetite', 4: 'B-son died 3 weeks ago', 5: 'I-son died 3 weeks ago', 6: 'B-female', 7: 'I-female', 8: 'B-auditory hallucination once', 9: 'I-auditory hallucination once', 10: 'B-tossing and turning', 11: 'I-tossing and turning', 12: 'B-67 year', 13: 'I-67 year', 14: 'B-difficulty falling asleep', 15: 'I-difficulty falling asleep', 16: 'B-hallucinations after taking ambien', 17: 'I-hallucinations after taking ambien', 18: 'B-duration 3 weeks', 19: 'I-duration 3 weeks', 20: 'B-unsuccessful napping', 21: 'I-unsuccessful napping', 22: 'B-sleeping medication ineffective', 23: 'I-sleeping medication ineffective', 24: 'B-diminished energy or feeling drained', 25: 'I-diminished energy or feeling drained', 26: 'B-loss of interest', 27: 'I-loss of interest', 28: 'B-visual hallucination once', 29: 'I-visual hallucination once', 30: 'B-fhx of depression or family history of depression', 31: 'I-fhx of depression or family history of depression', 32: 'B-early wakening', 33: 'I-early wakening', 34: 'B-no suicidal ideations', 35: 'I-no suicidal ideations', 36: 'B-difficulty with sleep', 37: 'I-difficulty with sleep', 38: 'B-no relief with motrin or no relief with tylenol', 39: 'I-no relief with motrin or no relief with tylenol', 40: 'B-20 year', 41: 'I-20 year', 42: 'B-1 day duration or 2 days duration', 43: 'I-1 day duration or 2 days duration', 44: 'B-myalgias', 45: 'I-myalgias', 46: 'B-global headache or diffuse headache', 47: 'I-global headache or diffuse headache', 48: 'B-neck pain', 49: 'I-neck pain', 50: 'B-vomiting', 51: 'I-vomiting', 52: 'B-no rash', 53: 'I-no rash', 54: 'B-nausea', 55: 'I-nausea', 56: 'B-viral symptoms or rhinorrhea or scratchy throat', 57: 'I-viral symptoms or rhinorrhea or scratchy throat', 58: 'B-shares an apartment', 59: 'I-shares an apartment', 60: 'B-meningococcal vaccine status unknown', 61: 'I-meningococcal vaccine status unknown', 62: 'B-family history of migraines', 63: 'I-family history of migraines', 64: 'B-photophobia', 65: 'I-photophobia', 66: 'B-no known illness contacts', 67: 'I-no known illness contacts', 68: 'B-subjective fever', 69: 'I-subjective fever', 70: 'B-fhx of pud or family history of peptic ulcer disease', 71: 'I-fhx of pud or family history of peptic ulcer disease', 72: 'B-epigastric discomfort', 73: 'I-epigastric discomfort', 74: 'B-darker bowel movements', 75: 'I-darker bowel movements', 76: 'B-nsaid use or nonsteroidal anti inflammatory drug use', 77: 'I-nsaid use or nonsteroidal anti inflammatory drug use', 78: 'B-burning or gnawing or burning and gnawing', 79: 'I-burning or gnawing or burning and gnawing', 80: 'B-post prandial bloating or fullness with meals', 81: 'I-post prandial bloating or fullness with meals', 82: 'B-getting worse or progressive or symptoms now daily', 83: 'I-getting worse or progressive or symptoms now daily', 84: 'B-2 to 3 beers a week', 85: 'I-2 to 3 beers a week', 86: 'B-male', 87: 'I-male', 88: 'B-duration 2 months', 89: 'I-duration 2 months', 90: 'B-awakens at night', 91: 'I-awakens at night', 92: 'B-no blood in stool', 93: 'I-no blood in stool', 94: 'B-intermittent', 95: 'I-intermittent', 96: 'B-minimal to no change with tums', 97: 'I-minimal to no change with tums', 98: 'B-35 year', 99: 'I-35 year', 100: 'B-no vaginal discharge', 101: 'I-no vaginal discharge', 102: 'B-weight loss', 103: 'I-weight loss', 104: 'B-not sexually active', 105: 'I-not sexually active', 106: 'B-prior episodes of diarrhea', 107: 'I-prior episodes of diarrhea', 108: 'B-no bloody bowel movements', 109: 'I-no bloody bowel movements', 110: 'B-recurrent bouts over past 6 months', 111: 'I-recurrent bouts over past 6 months', 112: 'B-right sided lq abdominal pain or right lower quadrant abdominal pain', 113: 'I-right sided lq abdominal pain or right lower quadrant abdominal pain', 114: 'B-no urinary symptoms', 115: 'I-no urinary symptoms', 116: 'B-diminished appetite', 117: 'I-diminished appetite', 118: 'B-normal lmp 2 weeks ago or normal last menstrual period 2 weeks ago', 119: 'I-normal lmp 2 weeks ago or normal last menstrual period 2 weeks ago', 120: 'B-8 to 10 hours of acute pain', 121: 'I-8 to 10 hours of acute pain', 122: 'B-onset 5 years ago', 123: 'I-onset 5 years ago', 124: 'B-no caffeine use', 125: 'I-no caffeine use', 126: 'B-associated sob or associated shortness of breath', 127: 'I-associated sob or associated shortness of breath', 128: 'B-episodes of heart racing', 129: 'I-episodes of heart racing', 130: 'B-recent visit to emergency department with negative workup', 131: 'I-recent visit to emergency department with negative workup', 132: 'B-no chest pain', 133: 'I-no chest pain', 134: 'B-no illicit drug use', 135: 'I-no illicit drug use', 136: 'B-associated nausea', 137: 'I-associated nausea', 138: 'B-increased frequency recently', 139: 'I-increased frequency recently', 140: 'B-associated feeling of impending doom', 141: 'I-associated feeling of impending doom', 142: 'B-episodes last 15 to 30 minutes', 143: 'I-episodes last 15 to 30 minutes', 144: 'B-associated throat tightness', 145: 'I-associated throat tightness', 146: 'B-feels hot or feels clammy', 147: 'I-feels hot or feels clammy', 148: 'B-episode of hand numbness or episode of finger numbness', 149: 'I-episode of hand numbness or episode of finger numbness', 150: 'B-fatigue or difficulty concentrating', 151: 'I-fatigue or difficulty concentrating', 152: 'B-increased stress', 153: 'I-increased stress', 154: 'B-26 year', 155: 'I-26 year', 156: 'B-lack of other thyroid symptoms', 157: 'I-lack of other thyroid symptoms', 158: 'B-anxious or nervous', 159: 'I-anxious or nervous', 160: 'B-stress due to caring for elderly parents', 161: 'I-stress due to caring for elderly parents', 162: 'B-heavy caffeine use', 163: 'I-heavy caffeine use', 164: 'B-no depressed mood', 165: 'I-no depressed mood', 166: 'B-weight stable', 167: 'I-weight stable', 168: 'B-insomnia', 169: 'I-insomnia', 170: 'B-decreased appetite', 171: 'I-decreased appetite', 172: 'B-45 year', 173: 'I-45 year', 174: 'B-family history of mi or family history of myocardial infarction', 175: 'I-family history of mi or family history of myocardial infarction', 176: 'B-family history of thyroid disorder', 177: 'I-family history of thyroid disorder', 178: 'B-chest pressure', 179: 'I-chest pressure', 180: 'B-intermittent symptoms', 181: 'I-intermittent symptoms', 182: 'B-lightheaded', 183: 'I-lightheaded', 184: 'B-no hair changes or no nail changes or no temperature intolerance', 185: 'I-no hair changes or no nail changes or no temperature intolerance', 186: 'B-adderall use', 187: 'I-adderall use', 188: 'B-shortness of breath', 189: 'I-shortness of breath', 190: 'B-caffeine use', 191: 'I-caffeine use', 192: 'B-heart pounding or heart racing', 193: 'I-heart pounding or heart racing', 194: 'B-few months duration', 195: 'I-few months duration', 196: 'B-17 year', 197: 'I-17 year', 198: 'B-weight gain', 199: 'I-weight gain', 200: 'B-heavy periods or irregular periods', 201: 'I-heavy periods or irregular periods', 202: 'B-last menstrual period 2 months ago', 203: 'I-last menstrual period 2 months ago', 204: 'B-unprotected sex', 205: 'I-unprotected sex', 206: 'B-fatigue', 207: 'I-fatigue', 208: 'B-infertility hx or infertility history', 209: 'I-infertility hx or infertility history', 210: 'B-symptoms for 6 months', 211: 'I-symptoms for 6 months', 212: 'B-prior normal periods', 213: 'I-prior normal periods', 214: 'B-last pap smear i year ago', 215: 'I-last pap smear i year ago', 216: 'B-iud', 217: 'I-iud', 218: 'B-sexually active', 219: 'I-sexually active', 220: 'B-vaginal dryness', 221: 'I-vaginal dryness', 222: 'B-irregular menses', 223: 'I-irregular menses', 224: 'B-recent nausea vomiting or recent flulike symptoms', 225: 'I-recent nausea vomiting or recent flulike symptoms', 226: 'B-no premenstrual symptoms', 227: 'I-no premenstrual symptoms', 228: 'B-stress', 229: 'I-stress', 230: 'B-lmp 2 months ago or last menstrual period 2 months ago', 231: 'I-lmp 2 months ago or last menstrual period 2 months ago', 232: 'B-hot flashes', 233: 'I-hot flashes', 234: 'B-irregular flow or irregular frequency or irregular intervals', 235: 'I-irregular flow or irregular frequency or irregular intervals', 236: 'B-onset 3 years ago', 237: 'I-onset 3 years ago', 238: 'B-heavy sweating', 239: 'I-heavy sweating', 240: 'B-sleep disturbance or early awakenings', 241: 'I-sleep disturbance or early awakenings', 242: 'B-44 year', 243: 'I-44 year', 244: 'B-subjective fevers', 245: 'I-subjective fevers', 246: 'B-recent upper respiratory symptoms', 247: 'I-recent upper respiratory symptoms', 248: 'B-worse with deep breath or pleuritic', 249: 'I-worse with deep breath or pleuritic', 250: 'B-exercise induced asthma', 251: 'I-exercise induced asthma', 252: 'B-chest pain', 253: 'I-chest pain', 254: 'B-duration x 1 day', 255: 'I-duration x 1 day', 256: 'B-no shortness of breath', 257: 'I-no shortness of breath', 258: 'B-recent heavy lifting at work or recent rock climbing', 259: 'I-recent heavy lifting at work or recent rock climbing', 260: 'B-no relief with asthma inhaler', 261: 'I-no relief with asthma inhaler', 262: 'B-sharp or stabbing or 7 to 8 out of 10 on pain scale', 263: 'I-sharp or stabbing or 7 to 8 out of 10 on pain scale'}\n"
     ]
    }
   ],
   "source": [
    "def make_tag_lookup_table():\n",
    "    iob_labels = ['B', 'I']\n",
    "    ner_labels = list_of_targets\n",
    "    all_labels = [(label1, label2) for label2 in ner_labels for label1 in iob_labels]\n",
    "    all_labels = ['-'.join([a,b]) for a, b in all_labels]\n",
    "    all_labels = ['[PAD]', 'O'] + all_labels\n",
    "    return dict(zip(range(0, len(all_labels) + 1), all_labels))\n",
    "\n",
    "mapping = make_tag_lookup_table()\n",
    "print(mapping)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get  a list of all tokens in the training dataset. This will be used to create the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54380\n",
      "264\n"
     ]
    }
   ],
   "source": [
    "all_tokens = text_split\n",
    "all_tokens_array = np.array(' '.join(pd.Series(text)).split())\n",
    "\n",
    "counter = Counter(all_tokens_array)\n",
    "print(len(counter))\n",
    "\n",
    "num_tags = len(mapping)\n",
    "print(num_tags)\n",
    "vocab_size = 7000\n",
    "\n",
    "vocabulary = [token for token, count in counter.most_common(vocab_size - 2)]\n",
    "\n",
    "# The StringLook class will convert tokens to token IDs\n",
    "lookup_layer = keras.layers.StringLookup(\n",
    "    vocabulary=vocabulary\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0               67\n",
       "1               yo\n",
       "2                f\n",
       "3          present\n",
       "4             with\n",
       "            ...   \n",
       "6224087       etoh\n",
       "6224088     drinks\n",
       "6224089       2-3x\n",
       "6224090         wk\n",
       "6224091          .\n",
       "Length: 6224092, dtype: object"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(all_tokens_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[',', '.', ':', 'and', 'no', 'with', 'she', 'of', 'the', 'a']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create 2 new `Dataset` objects from the training and validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = tf.data.TextLineDataset('./data/train.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TextLineDatasetV2 element_spec=TensorSpec(shape=(), dtype=tf.string, name=None)>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print out one line to make sure it looks good. The first record in the line is the number of tokens. After that I will have all the tokens followed by all the ner tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'186\\t20yof\\tpresenting\\twith\\tha\\t.\\tshe\\tsaid\\tthe\\tha\\tstarted\\tyesterday\\tand\\thas\\tbeen\\ta\\tconstant\\tdull\\tpain\\tthat\\tis\\tworsening\\t.\\tshe\\tsays\\tthe\\tha\\tinvolved\\ther\\twhole\\thead\\t.\\tshe\\thas\\ttried\\ttaking\\tibuprofen\\t,\\ttylenol\\t,\\tand\\tresting\\tfor\\tthe\\tha\\tand\\tthese\\thave\\tnot\\tbeen\\thelpful\\t.\\tshe\\tsays\\tthe\\tha\\tis\\tworse\\twith\\twalking\\tand\\tbending\\tover\\t.\\tshe\\tsays\\tshe\\thas\\thad\\tnausea\\tand\\thas\\tvomited\\t3x\\t.\\tshe\\tis\\thaving\\tphotophobia\\tbut\\tno\\tphonophobia\\t.\\tshe\\tsays\\tshe\\thas\\ta\\trunny\\tnose\\tbut\\tdenies\\tany\\teye\\ttearing\\tor\\tredness\\t.\\tshe\\tsaid\\tshe\\tfelt\\twarm\\tbut\\tdid\\tnot\\tactually\\ttake\\ther\\ttemperature\\t.\\tshe\\tdenies\\tany\\tvisual\\tchanges\\t,\\tcp\\t,\\tsob\\t.\\tshe\\tdenies\\tany\\trecent\\tillnesses\\tor\\thead\\ttrauma\\t.\\tshe\\thas\\tnever\\thad\\tthis\\thappen\\tbefore\\t.\\tros\\t:\\tnegative\\texcept\\thpi\\tpmh\\t:\\tnone\\tmeds\\t:\\tocp\\tnkda\\tfh\\t:\\tmom-\\tmigraines\\tsh\\t:\\tno\\ttobacco\\tuse\\t,\\tetoh\\t2-3\\tweek\\t,\\tsmokes\\tmarijuana\\t3-4\\ttimes\\tweek\\t,\\tsexually\\tactive\\twith\\tboyfriend\\tand\\tuse\\tcondoms\\t,\\tdenies\\thistory\\tof\\tstds\\t,\\tlmp\\t2\\tweeks\\tago\\t2523\\t187\\t7\\t402\\t3\\t8\\t1063\\t10\\t402\\t98\\t165\\t5\\t13\\t61\\t11\\t180\\t264\\t12\\t30\\t15\\t342\\t3\\t8\\t313\\t10\\t402\\t2383\\t21\\t1152\\t314\\t3\\t8\\t13\\t190\\t243\\t220\\t2\\t130\\t2\\t5\\t2005\\t16\\t10\\t402\\t5\\t226\\t112\\t24\\t61\\t1494\\t3\\t8\\t313\\t10\\t402\\t15\\t84\\t7\\t308\\t5\\t348\\t111\\t3\\t8\\t313\\t8\\t13\\t34\\t48\\t5\\t13\\t726\\t761\\t3\\t8\\t15\\t200\\t441\\t25\\t6\\t1106\\t3\\t8\\t313\\t8\\t13\\t11\\t521\\t422\\t25\\t20\\t58\\t1199\\t1584\\t18\\t2564\\t3\\t8\\t1063\\t8\\t370\\t511\\t25\\t346\\t24\\t1989\\t515\\t21\\t728\\t3\\t8\\t20\\t58\\t468\\t32\\t2\\t445\\t2\\t92\\t3\\t8\\t20\\t58\\t125\\t990\\t18\\t314\\t319\\t3\\t8\\t13\\t188\\t34\\t94\\t581\\t189\\t3\\t38\\t4\\t88\\t103\\t123\\t28\\t4\\t19\\t40\\t4\\t483\\t75\\t53\\t4\\t1553\\t396\\t60\\t4\\t6\\t114\\t36\\t2\\t127\\t129\\t39\\t2'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(train_data.as_numpy_iterator())[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I will use the following map function to transform the data in the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_record_to_training_data(record):\n",
    "    record = tf.strings.split(record, sep='\\t')\n",
    "    length = tf.strings.to_number(record[0], out_type=tf.int32)\n",
    "    tokens = record[1 : length + 1]\n",
    "    tags = record[length + 1 :]\n",
    "    tags = tf.strings.to_number(tags, out_type=tf.int64)\n",
    "    tags += 1\n",
    "    return tokens, tags\n",
    "\n",
    "def lowercase_and_convert_to_ids(tokens):\n",
    "    tokens = tf.strings.lower(tokens)\n",
    "    return lookup_layer(tokens)\n",
    "\n",
    "# I use `padded_batch` here because each record in the dataset has a different length.\n",
    "batch_size = 169\n",
    "train_dataset = (\n",
    "    train_data.map(map_record_to_training_data)\n",
    "    .map(lambda x, y: (lowercase_and_convert_to_ids(x), y))\n",
    "    .padded_batch(batch_size)\n",
    ")\n",
    "\n",
    "ner_model = NERModel(num_tags, vocab_size, embed_dim=170, num_heads = 3, ff_dim = 170)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PaddedBatchDataset element_spec=(TensorSpec(shape=(None, None), dtype=tf.int64, name=None), TensorSpec(shape=(None, None), dtype=tf.int64, name=None))>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I will use a custom loss function that will ignore the loss from padded tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomNonPaddingTokenLoss(keras.losses.Loss):\n",
    "    def __init__(self, name='custom_ner_loss'):\n",
    "        super().__init__(name=name)\n",
    "\n",
    "    def call(self, y_true, y_pred):\n",
    "        loss_fn = keras.losses.SparseCategoricalCrossentropy(\n",
    "            from_logits=True, reduction=keras.losses.Reduction.NONE\n",
    "        )\n",
    "        loss = loss_fn(y_true, y_pred)\n",
    "        mask = tf.cast((y_true > 0), dtype=tf.float32)\n",
    "        loss = loss * mask\n",
    "        return tf.reduce_sum(loss) / tf.reduce_sum(mask)\n",
    "\n",
    "loss = CustomNonPaddingTokenLoss()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile and fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Graph execution error:\n\nDetected at node 'ner_model_2/token_and_position_embedding_2/embedding_6/embedding_lookup' defined at (most recent call last):\n    File \"/usr/local/anaconda3/lib/python3.8/runpy.py\", line 194, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"/usr/local/anaconda3/lib/python3.8/runpy.py\", line 87, in _run_code\n      exec(code, run_globals)\n    File \"/usr/local/anaconda3/lib/python3.8/site-packages/ipykernel_launcher.py\", line 16, in <module>\n      app.launch_new_instance()\n    File \"/usr/local/anaconda3/lib/python3.8/site-packages/traitlets/config/application.py\", line 845, in launch_instance\n      app.start()\n    File \"/usr/local/anaconda3/lib/python3.8/site-packages/ipykernel/kernelapp.py\", line 677, in start\n      self.io_loop.start()\n    File \"/usr/local/anaconda3/lib/python3.8/site-packages/tornado/platform/asyncio.py\", line 199, in start\n      self.asyncio_loop.run_forever()\n    File \"/usr/local/anaconda3/lib/python3.8/asyncio/base_events.py\", line 570, in run_forever\n      self._run_once()\n    File \"/usr/local/anaconda3/lib/python3.8/asyncio/base_events.py\", line 1859, in _run_once\n      handle._run()\n    File \"/usr/local/anaconda3/lib/python3.8/asyncio/events.py\", line 81, in _run\n      self._context.run(self._callback, *self._args)\n    File \"/usr/local/anaconda3/lib/python3.8/site-packages/ipykernel/kernelbase.py\", line 457, in dispatch_queue\n      await self.process_one()\n    File \"/usr/local/anaconda3/lib/python3.8/site-packages/ipykernel/kernelbase.py\", line 446, in process_one\n      await dispatch(*args)\n    File \"/usr/local/anaconda3/lib/python3.8/site-packages/ipykernel/kernelbase.py\", line 353, in dispatch_shell\n      await result\n    File \"/usr/local/anaconda3/lib/python3.8/site-packages/ipykernel/kernelbase.py\", line 648, in execute_request\n      reply_content = await reply_content\n    File \"/usr/local/anaconda3/lib/python3.8/site-packages/ipykernel/ipkernel.py\", line 353, in do_execute\n      res = shell.run_cell(code, store_history=store_history, silent=silent)\n    File \"/usr/local/anaconda3/lib/python3.8/site-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n      return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n    File \"/usr/local/anaconda3/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 2901, in run_cell\n      result = self._run_cell(\n    File \"/usr/local/anaconda3/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 2947, in _run_cell\n      return runner(coro)\n    File \"/usr/local/anaconda3/lib/python3.8/site-packages/IPython/core/async_helpers.py\", line 68, in _pseudo_sync_runner\n      coro.send(None)\n    File \"/usr/local/anaconda3/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3172, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"/usr/local/anaconda3/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3364, in run_ast_nodes\n      if (await self.run_code(code, result,  async_=asy)):\n    File \"/usr/local/anaconda3/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3444, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"/var/folders/jl/s3ptdwdx55v01d2g2wrs7vdc0000gn/T/ipykernel_7779/3145190542.py\", line 2, in <module>\n      ner_model.fit(train_dataset, epochs = 10)\n    File \"/usr/local/anaconda3/lib/python3.8/site-packages/keras/utils/traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"/usr/local/anaconda3/lib/python3.8/site-packages/keras/engine/training.py\", line 1384, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"/usr/local/anaconda3/lib/python3.8/site-packages/keras/engine/training.py\", line 1021, in train_function\n      return step_function(self, iterator)\n    File \"/usr/local/anaconda3/lib/python3.8/site-packages/keras/engine/training.py\", line 1010, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/anaconda3/lib/python3.8/site-packages/keras/engine/training.py\", line 1000, in run_step\n      outputs = model.train_step(data)\n    File \"/usr/local/anaconda3/lib/python3.8/site-packages/keras/engine/training.py\", line 859, in train_step\n      y_pred = self(x, training=True)\n    File \"/usr/local/anaconda3/lib/python3.8/site-packages/keras/utils/traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"/usr/local/anaconda3/lib/python3.8/site-packages/keras/engine/base_layer.py\", line 1096, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"/usr/local/anaconda3/lib/python3.8/site-packages/keras/utils/traceback_utils.py\", line 92, in error_handler\n      return fn(*args, **kwargs)\n    File \"/var/folders/jl/s3ptdwdx55v01d2g2wrs7vdc0000gn/T/ipykernel_7779/832956628.py\", line 14, in call\n      x = self.embedding_layer(inputs)\n    File \"/usr/local/anaconda3/lib/python3.8/site-packages/keras/utils/traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"/usr/local/anaconda3/lib/python3.8/site-packages/keras/engine/base_layer.py\", line 1096, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"/usr/local/anaconda3/lib/python3.8/site-packages/keras/utils/traceback_utils.py\", line 92, in error_handler\n      return fn(*args, **kwargs)\n    File \"/var/folders/jl/s3ptdwdx55v01d2g2wrs7vdc0000gn/T/ipykernel_7779/3882870019.py\", line 10, in call\n      position_embeddings = self.pos_emb(positions)\n    File \"/usr/local/anaconda3/lib/python3.8/site-packages/keras/utils/traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"/usr/local/anaconda3/lib/python3.8/site-packages/keras/engine/base_layer.py\", line 1096, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"/usr/local/anaconda3/lib/python3.8/site-packages/keras/utils/traceback_utils.py\", line 92, in error_handler\n      return fn(*args, **kwargs)\n    File \"/usr/local/anaconda3/lib/python3.8/site-packages/keras/layers/embeddings.py\", line 197, in call\n      out = tf.nn.embedding_lookup(self.embeddings, inputs)\nNode: 'ner_model_2/token_and_position_embedding_2/embedding_6/embedding_lookup'\nindices[170] = 170 is not in [0, 170)\n\t [[{{node ner_model_2/token_and_position_embedding_2/embedding_6/embedding_lookup}}]] [Op:__inference_train_function_123030]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/jl/s3ptdwdx55v01d2g2wrs7vdc0000gn/T/ipykernel_7779/3145190542.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mner_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'adam'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mner_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtokenize_and_convert_to_ids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.8/site-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     52\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     55\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     56\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Graph execution error:\n\nDetected at node 'ner_model_2/token_and_position_embedding_2/embedding_6/embedding_lookup' defined at (most recent call last):\n    File \"/usr/local/anaconda3/lib/python3.8/runpy.py\", line 194, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"/usr/local/anaconda3/lib/python3.8/runpy.py\", line 87, in _run_code\n      exec(code, run_globals)\n    File \"/usr/local/anaconda3/lib/python3.8/site-packages/ipykernel_launcher.py\", line 16, in <module>\n      app.launch_new_instance()\n    File \"/usr/local/anaconda3/lib/python3.8/site-packages/traitlets/config/application.py\", line 845, in launch_instance\n      app.start()\n    File \"/usr/local/anaconda3/lib/python3.8/site-packages/ipykernel/kernelapp.py\", line 677, in start\n      self.io_loop.start()\n    File \"/usr/local/anaconda3/lib/python3.8/site-packages/tornado/platform/asyncio.py\", line 199, in start\n      self.asyncio_loop.run_forever()\n    File \"/usr/local/anaconda3/lib/python3.8/asyncio/base_events.py\", line 570, in run_forever\n      self._run_once()\n    File \"/usr/local/anaconda3/lib/python3.8/asyncio/base_events.py\", line 1859, in _run_once\n      handle._run()\n    File \"/usr/local/anaconda3/lib/python3.8/asyncio/events.py\", line 81, in _run\n      self._context.run(self._callback, *self._args)\n    File \"/usr/local/anaconda3/lib/python3.8/site-packages/ipykernel/kernelbase.py\", line 457, in dispatch_queue\n      await self.process_one()\n    File \"/usr/local/anaconda3/lib/python3.8/site-packages/ipykernel/kernelbase.py\", line 446, in process_one\n      await dispatch(*args)\n    File \"/usr/local/anaconda3/lib/python3.8/site-packages/ipykernel/kernelbase.py\", line 353, in dispatch_shell\n      await result\n    File \"/usr/local/anaconda3/lib/python3.8/site-packages/ipykernel/kernelbase.py\", line 648, in execute_request\n      reply_content = await reply_content\n    File \"/usr/local/anaconda3/lib/python3.8/site-packages/ipykernel/ipkernel.py\", line 353, in do_execute\n      res = shell.run_cell(code, store_history=store_history, silent=silent)\n    File \"/usr/local/anaconda3/lib/python3.8/site-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n      return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n    File \"/usr/local/anaconda3/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 2901, in run_cell\n      result = self._run_cell(\n    File \"/usr/local/anaconda3/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 2947, in _run_cell\n      return runner(coro)\n    File \"/usr/local/anaconda3/lib/python3.8/site-packages/IPython/core/async_helpers.py\", line 68, in _pseudo_sync_runner\n      coro.send(None)\n    File \"/usr/local/anaconda3/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3172, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"/usr/local/anaconda3/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3364, in run_ast_nodes\n      if (await self.run_code(code, result,  async_=asy)):\n    File \"/usr/local/anaconda3/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3444, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"/var/folders/jl/s3ptdwdx55v01d2g2wrs7vdc0000gn/T/ipykernel_7779/3145190542.py\", line 2, in <module>\n      ner_model.fit(train_dataset, epochs = 10)\n    File \"/usr/local/anaconda3/lib/python3.8/site-packages/keras/utils/traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"/usr/local/anaconda3/lib/python3.8/site-packages/keras/engine/training.py\", line 1384, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"/usr/local/anaconda3/lib/python3.8/site-packages/keras/engine/training.py\", line 1021, in train_function\n      return step_function(self, iterator)\n    File \"/usr/local/anaconda3/lib/python3.8/site-packages/keras/engine/training.py\", line 1010, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/anaconda3/lib/python3.8/site-packages/keras/engine/training.py\", line 1000, in run_step\n      outputs = model.train_step(data)\n    File \"/usr/local/anaconda3/lib/python3.8/site-packages/keras/engine/training.py\", line 859, in train_step\n      y_pred = self(x, training=True)\n    File \"/usr/local/anaconda3/lib/python3.8/site-packages/keras/utils/traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"/usr/local/anaconda3/lib/python3.8/site-packages/keras/engine/base_layer.py\", line 1096, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"/usr/local/anaconda3/lib/python3.8/site-packages/keras/utils/traceback_utils.py\", line 92, in error_handler\n      return fn(*args, **kwargs)\n    File \"/var/folders/jl/s3ptdwdx55v01d2g2wrs7vdc0000gn/T/ipykernel_7779/832956628.py\", line 14, in call\n      x = self.embedding_layer(inputs)\n    File \"/usr/local/anaconda3/lib/python3.8/site-packages/keras/utils/traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"/usr/local/anaconda3/lib/python3.8/site-packages/keras/engine/base_layer.py\", line 1096, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"/usr/local/anaconda3/lib/python3.8/site-packages/keras/utils/traceback_utils.py\", line 92, in error_handler\n      return fn(*args, **kwargs)\n    File \"/var/folders/jl/s3ptdwdx55v01d2g2wrs7vdc0000gn/T/ipykernel_7779/3882870019.py\", line 10, in call\n      position_embeddings = self.pos_emb(positions)\n    File \"/usr/local/anaconda3/lib/python3.8/site-packages/keras/utils/traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"/usr/local/anaconda3/lib/python3.8/site-packages/keras/engine/base_layer.py\", line 1096, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"/usr/local/anaconda3/lib/python3.8/site-packages/keras/utils/traceback_utils.py\", line 92, in error_handler\n      return fn(*args, **kwargs)\n    File \"/usr/local/anaconda3/lib/python3.8/site-packages/keras/layers/embeddings.py\", line 197, in call\n      out = tf.nn.embedding_lookup(self.embeddings, inputs)\nNode: 'ner_model_2/token_and_position_embedding_2/embedding_6/embedding_lookup'\nindices[170] = 170 is not in [0, 170)\n\t [[{{node ner_model_2/token_and_position_embedding_2/embedding_6/embedding_lookup}}]] [Op:__inference_train_function_123030]"
     ]
    }
   ],
   "source": [
    "ner_model.compile(optimizer='adam', loss = loss)\n",
    "ner_model.fit(train_dataset, epochs = 10)\n",
    "\n",
    "def tokenize_and_convert_to_ids(text):\n",
    "    tokens = text.split()\n",
    "    return lowercase_and_convert_to_ids(tokens)\n",
    "\n",
    "# Sample inference using the trained model\n",
    "sample_input = tokenize_and_convert_to_ids(\n",
    "    '67 yo f present with trouble sleeping .  pt states that it began 3 weeks ago and describes as difficulty falling asleep ,  waking up early ,  and tossing and turning at night .  never had this problem before ,  tried taking a friends ambien ,  and it did not help .  pts son died on aug 17 in an mva .  pt states that she has a sad mood ,  loss of interest in activites ,  decreased energy level ,  difficulty sleeping ,  increased appetite ,  and hallucinations of seeing son ,  and hearing people next door .     ros :  negative except for above  allergies :  none  meds hctz 25 mg qd ,  lisinopril 20 mg qd  pmh :  htn (15 years) ,  in remission for breast cancer (10 years) ,  ruptured appendicitis  pshx :  lumpectomy ,  laperatomy in 20s  fh :  father with htn ,  hypercholesterolemia ,  died of stroke ,  mother with dm  sh :  married (45 years) ,  drinks etoh 2-3 times per week (-) cage ,  good support system ,  denies tobacco and recreational drug use'\n",
    ")\n",
    "sample_input = tf.reshape(sample_input, shape=[1, -1])\n",
    "print(sample_input)\n",
    "\n",
    "output = ner_model.predict(sample_input)\n",
    "prediction = np.argmax(output, axis=-1)[0]\n",
    "prediction = [mapping[i] for i in prediction]\n",
    "\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3067ead486e059ec00ffe7555bdb889e6e264a24dc711bf108106cc7baee8d5d"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
